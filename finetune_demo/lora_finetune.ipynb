{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89b89f64d8f8053d",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 单卡GPU 进行 ChatGLM3-6B模型 LORA 高效微调\n",
    "本 Cookbook 将带领开发者使用 `AdvertiseGen` 对 ChatGLM3-6B 数据集进行 lora微调，使其具备专业的广告生成能力。\n",
    "\n",
    "## 硬件需求\n",
    "显存：24GB\n",
    "显卡架构：安培架构（推荐）\n",
    "内存：16GB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7bd9a514ed09ea6",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 1. 准备数据集\n",
    "我们使用 AdvertiseGen 数据集来进行微调。从 [Google Drive](https://drive.google.com/file/d/13_vf0xRTQsyneRKdD1bZIr93vBGOczrk/view?usp=sharing) 或者 [Tsinghua Cloud](https://cloud.tsinghua.edu.cn/f/b3f119a008264b1cabd1/?dl=1) 下载处理好的 AdvertiseGen 数据集，将解压后的 AdvertiseGen 目录放到本目录的 `/data/` 下, 例如。\n",
    "> /media/zr/Data/Code/ChatGLM3/finetune_demo/data/AdvertiseGen\n",
    "\n",
    "接着，运行本代码来切割数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-18T05:02:34.749308Z",
     "start_time": "2024-01-18T05:02:25.564458Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import Union\n",
    "from pathlib import Path\n",
    "\n",
    "# 将路径转换为Path对象，并解析环境变量和用户路径\n",
    "def _resolve_path(path: Union[str, Path]) -> Path: \n",
    "    return Path(path).expanduser().resolve()\n",
    "\n",
    "\n",
    "def _mkdir(dir_name: Union[str, Path]):\n",
    "    # 解析路径\n",
    "    dir_name = _resolve_path(dir_name)\n",
    "    # 检查路径是否为目录\n",
    "    if not dir_name.is_dir():\n",
    "        # 创建目录，如果有父目录，则创建父目录\n",
    "        dir_name.mkdir(parents=True, exist_ok=False)\n",
    "\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Union\n",
    "\n",
    "def convert_adgen(data_dir: Union[str, Path], save_dir: Union[str, Path]):\n",
    "\n",
    "    # 将AdvertiseGen数据集转换为fix格式\n",
    "    def _convert(in_file: Path, out_file: Path):\n",
    "\n",
    "        # 将一行的数据转换为fix格式的数据\n",
    "        _mkdir(out_file.parent)\n",
    "        with open(in_file, encoding='utf-8') as fin:\n",
    "            with open(out_file, 'wt', encoding='utf-8') as fout:\n",
    "                for line in fin:\n",
    "                    dct = json.loads(line)\n",
    "                    sample = {'conversations': [{'role': 'user', 'content': dct['content']},\n",
    "                                                {'role': 'assistant', 'content': dct['summary']}]}\n",
    "                    fout.write(json.dumps(sample, ensure_ascii=False) + '\\n')\n",
    "\n",
    "    data_dir = _resolve_path(data_dir)\n",
    "    save_dir = _resolve_path(save_dir)\n",
    "\n",
    "    train_file = data_dir / 'train.json'\n",
    "    if train_file.is_file():\n",
    "        out_file = save_dir / train_file.relative_to(data_dir)\n",
    "        _convert(train_file, out_file)\n",
    "\n",
    "    dev_file = data_dir / 'dev.json'\n",
    "    if dev_file.is_file():\n",
    "        out_file = save_dir / dev_file.relative_to(data_dir)\n",
    "        _convert(dev_file, out_file)\n",
    "\n",
    "\n",
    "convert_adgen('data/AdvertiseGen', 'data/AdvertiseGen_fix')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b7a99923349056",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 2. 使用命令行开始微调,我们使用 lora 进行微调\n",
    "接着，我们仅需要将配置好的参数以命令行的形式传参给程序，就可以使用命令行进行高效微调，这里将 `/media/zr/Data/Code/ChatGLM3/venv/bin/python3` 换成你的 python3 的绝对路径以保证正常运行。\n",
    "\n",
    "- python3的绝对路径：/root/miniconda3/envs/chatglm/bin/python3\n",
    "- chatglm3-6b的绝对路径：/root/models/chatglm3-6b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17c87410a24d844f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-18T06:44:56.043246Z",
     "start_time": "2024-01-18T05:05:28.425374Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting eos_token is not supported, use the default one.\n",
      "Setting pad_token is not supported, use the default one.\n",
      "Setting unk_token is not supported, use the default one.\n",
      "Loading checkpoint shards: 100%|██████████████████| 7/7 [00:03<00:00,  2.18it/s]\n",
      "trainable params: 1,949,696 || all params: 6,245,533,696 || trainable%: 0.031217444255383614\n",
      "--> Model\n",
      "\n",
      "--> model has 1.949696M params\n",
      "\n",
      "Map (num_proc=8): 100%|███████| 114599/114599 [00:04<00:00, 24484.36 examples/s]\n",
      "train_dataset: Dataset({\n",
      "    features: ['input_ids', 'labels'],\n",
      "    num_rows: 114599\n",
      "})\n",
      "Map (num_proc=8): 100%|████████████| 1070/1070 [00:00<00:00, 2007.32 examples/s]\n",
      "val_dataset: Dataset({\n",
      "    features: ['input_ids', 'output_ids'],\n",
      "    num_rows: 1070\n",
      "})\n",
      "Map (num_proc=8): 100%|████████████| 1070/1070 [00:00<00:00, 1875.02 examples/s]\n",
      "test_dataset: Dataset({\n",
      "    features: ['input_ids', 'output_ids'],\n",
      "    num_rows: 1070\n",
      "})\n",
      "--> Sanity check\n",
      "           '[gMASK]': 64790 -> -100\n",
      "               'sop': 64792 -> -100\n",
      "          '<|user|>': 64795 -> -100\n",
      "                  '': 30910 -> -100\n",
      "                '\\n': 13 -> -100\n",
      "                  '': 30910 -> -100\n",
      "                '类型': 33467 -> -100\n",
      "                 '#': 31010 -> -100\n",
      "                 '裤': 56532 -> -100\n",
      "                 '*': 30998 -> -100\n",
      "                 '版': 55090 -> -100\n",
      "                 '型': 54888 -> -100\n",
      "                 '#': 31010 -> -100\n",
      "                '宽松': 40833 -> -100\n",
      "                 '*': 30998 -> -100\n",
      "                '风格': 32799 -> -100\n",
      "                 '#': 31010 -> -100\n",
      "                '性感': 40589 -> -100\n",
      "                 '*': 30998 -> -100\n",
      "                '图案': 37505 -> -100\n",
      "                 '#': 31010 -> -100\n",
      "                '线条': 37216 -> -100\n",
      "                 '*': 30998 -> -100\n",
      "                 '裤': 56532 -> -100\n",
      "                 '型': 54888 -> -100\n",
      "                 '#': 31010 -> -100\n",
      "                 '阔': 56529 -> -100\n",
      "                 '腿': 56158 -> -100\n",
      "                 '裤': 56532 -> -100\n",
      "     '<|assistant|>': 64796 -> -100\n",
      "                  '': 30910 -> 30910\n",
      "                '\\n': 13 -> 13\n",
      "                  '': 30910 -> 30910\n",
      "                '宽松': 40833 -> 40833\n",
      "                 '的': 54530 -> 54530\n",
      "                 '阔': 56529 -> 56529\n",
      "                 '腿': 56158 -> 56158\n",
      "                 '裤': 56532 -> 56532\n",
      "                 '这': 54551 -> 54551\n",
      "                '两年': 33808 -> 33808\n",
      "                '真的': 32041 -> 32041\n",
      "                 '吸': 55360 -> 55360\n",
      "                 '粉': 55486 -> 55486\n",
      "                '不少': 32138 -> 32138\n",
      "                 '，': 31123 -> 31123\n",
      "                '明星': 32943 -> 32943\n",
      "                '时尚': 33481 -> 33481\n",
      "                 '达': 54880 -> 54880\n",
      "                '人的': 31664 -> 31664\n",
      "                '心头': 46565 -> 46565\n",
      "                 '爱': 54799 -> 54799\n",
      "                 '。': 31155 -> 31155\n",
      "                '毕竟': 33051 -> 33051\n",
      "                 '好': 54591 -> 54591\n",
      "                 '穿': 55432 -> 55432\n",
      "                '时尚': 33481 -> 33481\n",
      "                 '，': 31123 -> 31123\n",
      "                 '谁': 55622 -> 55622\n",
      "                '都能': 32904 -> 32904\n",
      "                 '穿': 55432 -> 55432\n",
      "                 '出': 54557 -> 54557\n",
      "                 '腿': 56158 -> 56158\n",
      "                 '长': 54625 -> 54625\n",
      "                 '2': 30943 -> 30943\n",
      "                 '米': 55055 -> 55055\n",
      "               '的效果': 35590 -> 35590\n",
      "                '宽松': 40833 -> 40833\n",
      "                 '的': 54530 -> 54530\n",
      "                 '裤': 56532 -> 56532\n",
      "                 '腿': 56158 -> 56158\n",
      "                 '，': 31123 -> 31123\n",
      "               '当然是': 48466 -> 48466\n",
      "                 '遮': 57148 -> 57148\n",
      "                 '肉': 55343 -> 55343\n",
      "                 '小': 54603 -> 54603\n",
      "                '能手': 49355 -> 49355\n",
      "                 '啊': 55674 -> 55674\n",
      "                 '。': 31155 -> 31155\n",
      "                '上身': 51605 -> 51605\n",
      "                 '随': 55119 -> 55119\n",
      "                 '性': 54642 -> 54642\n",
      "                '自然': 31799 -> 31799\n",
      "                 '不': 54535 -> 54535\n",
      "                 '拘': 57036 -> 57036\n",
      "                 '束': 55625 -> 55625\n",
      "                 '，': 31123 -> 31123\n",
      "                '面料': 46839 -> 46839\n",
      "                 '亲': 55113 -> 55113\n",
      "                 '肤': 56089 -> 56089\n",
      "                '舒适': 33894 -> 33894\n",
      "                 '贴': 55778 -> 55778\n",
      "                '身体': 31902 -> 31902\n",
      "                 '验': 55017 -> 55017\n",
      "                 '感': 54706 -> 54706\n",
      "                 '棒': 56382 -> 56382\n",
      "                 '棒': 56382 -> 56382\n",
      "                 '哒': 59230 -> 59230\n",
      "                 '。': 31155 -> 31155\n",
      "                 '系': 54712 -> 54712\n",
      "                 '带': 54882 -> 54882\n",
      "                '部分': 31726 -> 31726\n",
      "                '增加': 31917 -> 31917\n",
      "                '设计': 31735 -> 31735\n",
      "                '看点': 45032 -> 45032\n",
      "                 '，': 31123 -> 31123\n",
      "                 '还': 54656 -> 54656\n",
      "                 '让': 54772 -> 54772\n",
      "                '单品': 46539 -> 46539\n",
      "               '的设计': 34481 -> 34481\n",
      "                 '感': 54706 -> 54706\n",
      "                '更强': 43084 -> 43084\n",
      "                 '。': 31155 -> 31155\n",
      "                '腿部': 46799 -> 46799\n",
      "                '线条': 37216 -> 37216\n",
      "                 '若': 55351 -> 55351\n",
      "                 '隐': 55733 -> 55733\n",
      "                 '若': 55351 -> 55351\n",
      "                 '现': 54600 -> 54600\n",
      "                 '的': 54530 -> 54530\n",
      "                 '，': 31123 -> 31123\n",
      "                '性感': 40589 -> 40589\n",
      "                 '撩': 58521 -> 58521\n",
      "                 '人': 54533 -> 54533\n",
      "                 '。': 31155 -> 31155\n",
      "                '颜色': 33692 -> 33692\n",
      "                 '敲': 57004 -> 57004\n",
      "                '温柔': 34678 -> 34678\n",
      "                 '的': 54530 -> 54530\n",
      "                 '，': 31123 -> 31123\n",
      "                 '与': 54619 -> 54619\n",
      "                '裤子': 44722 -> 44722\n",
      "                '本身': 32754 -> 32754\n",
      "                 '所': 54626 -> 54626\n",
      "                '呈现': 33169 -> 33169\n",
      "               '的风格': 48084 -> 48084\n",
      "                '有点': 33149 -> 33149\n",
      "                 '反': 54955 -> 54955\n",
      "                 '差': 55342 -> 55342\n",
      "                 '萌': 56842 -> 56842\n",
      "                 '。': 31155 -> 31155\n",
      "                  '': 2 -> 2\n",
      "/root/miniconda3/envs/chatglm/lib/python3.11/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n",
      "max_steps is given, it will override any value given in num_train_epochs\n",
      "***** Running training *****\n",
      "  Num examples = 114,599\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 4\n",
      "  Total optimization steps = 3,000\n",
      "  Number of trainable parameters = 1,949,696\n",
      "{'loss': 4.8886, 'grad_norm': 2.1207523345947266, 'learning_rate': 3.3333333333333335e-05, 'epoch': 0.0}\n",
      "{'loss': 4.6989, 'grad_norm': 2.954498052597046, 'learning_rate': 6.666666666666667e-05, 'epoch': 0.0}\n",
      "{'loss': 4.5946, 'grad_norm': 3.1632308959960938, 'learning_rate': 0.0001, 'epoch': 0.0}\n",
      "{'loss': 4.0347, 'grad_norm': 3.4362094402313232, 'learning_rate': 0.00013333333333333334, 'epoch': 0.0}\n",
      "{'loss': 3.9515, 'grad_norm': 2.8838982582092285, 'learning_rate': 0.00016666666666666666, 'epoch': 0.0}\n",
      "{'loss': 3.7036, 'grad_norm': 3.665698289871216, 'learning_rate': 0.0002, 'epoch': 0.0}\n",
      "{'loss': 3.6624, 'grad_norm': 3.913419246673584, 'learning_rate': 0.00023333333333333333, 'epoch': 0.0}\n",
      "{'loss': 3.6552, 'grad_norm': 3.8217344284057617, 'learning_rate': 0.0002666666666666667, 'epoch': 0.0}\n",
      "{'loss': 3.5375, 'grad_norm': 4.399274826049805, 'learning_rate': 0.0003, 'epoch': 0.0}\n",
      "{'loss': 3.6069, 'grad_norm': 3.796985149383545, 'learning_rate': 0.0003333333333333333, 'epoch': 0.0}\n",
      "{'loss': 3.5162, 'grad_norm': 4.126388072967529, 'learning_rate': 0.00036666666666666667, 'epoch': 0.0}\n",
      "{'loss': 3.7529, 'grad_norm': 3.8217337131500244, 'learning_rate': 0.0004, 'epoch': 0.0}\n",
      "{'loss': 3.5213, 'grad_norm': 3.2842612266540527, 'learning_rate': 0.00043333333333333337, 'epoch': 0.0}\n",
      "{'loss': 3.6044, 'grad_norm': 3.6786458492279053, 'learning_rate': 0.00046666666666666666, 'epoch': 0.0}\n",
      "{'loss': 3.6196, 'grad_norm': 3.1994428634643555, 'learning_rate': 0.0005, 'epoch': 0.01}\n",
      "{'loss': 3.6591, 'grad_norm': 3.5001773834228516, 'learning_rate': 0.0005333333333333334, 'epoch': 0.01}\n",
      "{'loss': 3.5495, 'grad_norm': 2.9805819988250732, 'learning_rate': 0.0005666666666666667, 'epoch': 0.01}\n",
      "{'loss': 3.533, 'grad_norm': 2.963528633117676, 'learning_rate': 0.0006, 'epoch': 0.01}\n",
      "{'loss': 3.5353, 'grad_norm': 3.3191592693328857, 'learning_rate': 0.0006333333333333333, 'epoch': 0.01}\n",
      "{'loss': 3.5105, 'grad_norm': 2.618276596069336, 'learning_rate': 0.0006666666666666666, 'epoch': 0.01}\n",
      "{'loss': 3.5275, 'grad_norm': 2.9491472244262695, 'learning_rate': 0.0007, 'epoch': 0.01}\n",
      "{'loss': 3.5976, 'grad_norm': 2.727644920349121, 'learning_rate': 0.0007333333333333333, 'epoch': 0.01}\n",
      "{'loss': 3.5217, 'grad_norm': 2.774625778198242, 'learning_rate': 0.0007666666666666667, 'epoch': 0.01}\n",
      "{'loss': 3.4601, 'grad_norm': 2.7284038066864014, 'learning_rate': 0.0008, 'epoch': 0.01}\n",
      "{'loss': 3.4812, 'grad_norm': 3.2039501667022705, 'learning_rate': 0.0008333333333333334, 'epoch': 0.01}\n",
      "{'loss': 3.595, 'grad_norm': 2.8648595809936523, 'learning_rate': 0.0008666666666666667, 'epoch': 0.01}\n",
      "{'loss': 3.5498, 'grad_norm': 2.759880781173706, 'learning_rate': 0.0009000000000000001, 'epoch': 0.01}\n",
      "{'loss': 3.5873, 'grad_norm': 2.831728219985962, 'learning_rate': 0.0009333333333333333, 'epoch': 0.01}\n",
      "{'loss': 3.6305, 'grad_norm': 2.788949489593506, 'learning_rate': 0.0009666666666666667, 'epoch': 0.01}\n",
      "{'loss': 3.5249, 'grad_norm': 3.2562615871429443, 'learning_rate': 0.001, 'epoch': 0.01}\n",
      "{'loss': 3.4799, 'grad_norm': 2.929659843444824, 'learning_rate': 0.0009962962962962963, 'epoch': 0.01}\n",
      "{'loss': 3.6242, 'grad_norm': 2.9581449031829834, 'learning_rate': 0.0009925925925925927, 'epoch': 0.01}\n",
      "{'loss': 3.4265, 'grad_norm': 3.0170726776123047, 'learning_rate': 0.000988888888888889, 'epoch': 0.01}\n",
      "{'loss': 3.5386, 'grad_norm': 2.9114575386047363, 'learning_rate': 0.000985185185185185, 'epoch': 0.01}\n",
      "{'loss': 3.5121, 'grad_norm': 3.1760287284851074, 'learning_rate': 0.0009814814814814816, 'epoch': 0.01}\n",
      "{'loss': 3.6036, 'grad_norm': 2.755784511566162, 'learning_rate': 0.0009777777777777777, 'epoch': 0.01}\n",
      "{'loss': 3.3972, 'grad_norm': 2.7110157012939453, 'learning_rate': 0.0009740740740740741, 'epoch': 0.01}\n",
      "{'loss': 3.5412, 'grad_norm': 2.924238681793213, 'learning_rate': 0.0009703703703703704, 'epoch': 0.01}\n",
      "{'loss': 3.5668, 'grad_norm': 3.0620105266571045, 'learning_rate': 0.0009666666666666667, 'epoch': 0.01}\n",
      "{'loss': 3.4952, 'grad_norm': 3.219205856323242, 'learning_rate': 0.0009629629629629629, 'epoch': 0.01}\n",
      "{'loss': 3.7355, 'grad_norm': 3.0524673461914062, 'learning_rate': 0.0009592592592592593, 'epoch': 0.01}\n",
      "{'loss': 3.5176, 'grad_norm': 3.389535665512085, 'learning_rate': 0.0009555555555555556, 'epoch': 0.01}\n",
      "{'loss': 3.615, 'grad_norm': 3.056839942932129, 'learning_rate': 0.0009518518518518518, 'epoch': 0.02}\n",
      "{'loss': 3.4471, 'grad_norm': 3.1975886821746826, 'learning_rate': 0.0009481481481481482, 'epoch': 0.02}\n",
      "{'loss': 3.4616, 'grad_norm': 2.887758731842041, 'learning_rate': 0.0009444444444444445, 'epoch': 0.02}\n",
      "{'loss': 3.4118, 'grad_norm': 3.1237170696258545, 'learning_rate': 0.0009407407407407408, 'epoch': 0.02}\n",
      "{'loss': 3.5991, 'grad_norm': 3.2337191104888916, 'learning_rate': 0.000937037037037037, 'epoch': 0.02}\n",
      "{'loss': 3.4154, 'grad_norm': 3.1221091747283936, 'learning_rate': 0.0009333333333333333, 'epoch': 0.02}\n",
      "{'loss': 3.4271, 'grad_norm': 3.0816867351531982, 'learning_rate': 0.0009296296296296296, 'epoch': 0.02}\n",
      "{'loss': 3.593, 'grad_norm': 2.96679425239563, 'learning_rate': 0.000925925925925926, 'epoch': 0.02}\n",
      " 17%|██████▋                                 | 500/3000 [07:13<37:06,  1.12it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 4\n",
      "\n",
      "  0%|                                                    | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      " 15%|██████▊                                     | 2/13 [00:04<00:22,  2.07s/it]\u001b[A\n",
      " 23%|██████████▏                                 | 3/13 [00:20<01:18,  7.83s/it]\u001b[A\n",
      " 31%|█████████████▌                              | 4/13 [00:24<00:59,  6.64s/it]\u001b[A\n",
      " 38%|████████████████▉                           | 5/13 [00:28<00:44,  5.60s/it]\u001b[A\n",
      " 46%|████████████████████▎                       | 6/13 [00:44<01:03,  9.10s/it]\u001b[A\n",
      " 54%|███████████████████████▋                    | 7/13 [01:00<01:08, 11.38s/it]\u001b[A\n",
      " 62%|███████████████████████████                 | 8/13 [01:16<01:04, 12.91s/it]\u001b[A\n",
      " 69%|██████████████████████████████▍             | 9/13 [01:20<00:39,  9.96s/it]\u001b[A\n",
      " 77%|█████████████████████████████████          | 10/13 [01:24<00:24,  8.26s/it]\u001b[A\n",
      " 85%|████████████████████████████████████▍      | 11/13 [01:29<00:14,  7.15s/it]\u001b[A\n",
      " 92%|███████████████████████████████████████▋   | 12/13 [01:45<00:09,  9.80s/it]\u001b[A\n",
      "100%|███████████████████████████████████████████| 13/13 [01:48<00:00,  7.71s/it]\u001b[ABuilding prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.676 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "                                                                                \n",
      "\u001b[A{'eval_rouge-1': 31.71236, 'eval_rouge-2': 6.642968, 'eval_rouge-l': 22.616736, 'eval_bleu-4': 0.03076293389414064, 'eval_runtime': 125.5358, 'eval_samples_per_second': 0.398, 'eval_steps_per_second': 0.104, 'epoch': 0.02}\n",
      " 17%|██████▋                                 | 500/3000 [09:19<37:06,  1.12it/s]\n",
      "100%|███████████████████████████████████████████| 13/13 [01:48<00:00,  7.71s/it]\u001b[A\n",
      "                                                                                \u001b[ACheckpoint destination directory ./output/checkpoint-500 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Saving model checkpoint to ./output/checkpoint-500\n",
      "tokenizer config file saved in ./output/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-500/special_tokens_map.json\n",
      "{'loss': 3.3422, 'grad_norm': 2.798006772994995, 'learning_rate': 0.0009222222222222223, 'epoch': 0.02}\n",
      "{'loss': 3.5912, 'grad_norm': 3.246539354324341, 'learning_rate': 0.0009185185185185185, 'epoch': 0.02}\n",
      "{'loss': 3.5767, 'grad_norm': 2.9402403831481934, 'learning_rate': 0.0009148148148148149, 'epoch': 0.02}\n",
      "{'loss': 3.472, 'grad_norm': 2.917827844619751, 'learning_rate': 0.0009111111111111111, 'epoch': 0.02}\n",
      "{'loss': 3.5221, 'grad_norm': 2.6857378482818604, 'learning_rate': 0.0009074074074074074, 'epoch': 0.02}\n",
      "{'loss': 3.6511, 'grad_norm': 2.720039129257202, 'learning_rate': 0.0009037037037037037, 'epoch': 0.02}\n",
      "{'loss': 3.5, 'grad_norm': 2.910503387451172, 'learning_rate': 0.0009000000000000001, 'epoch': 0.02}\n",
      "{'loss': 3.4159, 'grad_norm': 2.9371743202209473, 'learning_rate': 0.0008962962962962963, 'epoch': 0.02}\n",
      "{'loss': 3.4231, 'grad_norm': 2.824643611907959, 'learning_rate': 0.0008925925925925926, 'epoch': 0.02}\n",
      "{'loss': 3.53, 'grad_norm': 3.0987792015075684, 'learning_rate': 0.0008888888888888888, 'epoch': 0.02}\n",
      "{'loss': 3.496, 'grad_norm': 3.0949628353118896, 'learning_rate': 0.0008851851851851853, 'epoch': 0.02}\n",
      "{'loss': 3.4714, 'grad_norm': 3.2770748138427734, 'learning_rate': 0.0008814814814814816, 'epoch': 0.02}\n",
      "{'loss': 3.4582, 'grad_norm': 2.961878538131714, 'learning_rate': 0.0008777777777777778, 'epoch': 0.02}\n",
      "{'loss': 3.4632, 'grad_norm': 2.924745559692383, 'learning_rate': 0.0008740740740740741, 'epoch': 0.02}\n",
      "{'loss': 3.5563, 'grad_norm': 2.56245493888855, 'learning_rate': 0.0008703703703703704, 'epoch': 0.02}\n",
      "{'loss': 3.4853, 'grad_norm': 2.9795644283294678, 'learning_rate': 0.0008666666666666667, 'epoch': 0.02}\n",
      "{'loss': 3.5306, 'grad_norm': 2.9877655506134033, 'learning_rate': 0.0008629629629629629, 'epoch': 0.02}\n",
      "{'loss': 3.2638, 'grad_norm': 2.7977449893951416, 'learning_rate': 0.0008592592592592593, 'epoch': 0.02}\n",
      "{'loss': 3.3978, 'grad_norm': 3.242657423019409, 'learning_rate': 0.0008555555555555556, 'epoch': 0.02}\n",
      "{'loss': 3.3765, 'grad_norm': 3.029435157775879, 'learning_rate': 0.0008518518518518519, 'epoch': 0.02}\n",
      "{'loss': 3.461, 'grad_norm': 2.930142879486084, 'learning_rate': 0.0008481481481481481, 'epoch': 0.02}\n",
      "{'loss': 3.5022, 'grad_norm': 2.896646022796631, 'learning_rate': 0.0008444444444444444, 'epoch': 0.03}\n",
      "{'loss': 3.2356, 'grad_norm': 3.1106433868408203, 'learning_rate': 0.0008407407407407409, 'epoch': 0.03}\n",
      "{'loss': 3.5479, 'grad_norm': 2.6395795345306396, 'learning_rate': 0.0008370370370370371, 'epoch': 0.03}\n",
      "{'loss': 3.3874, 'grad_norm': 2.7116992473602295, 'learning_rate': 0.0008333333333333334, 'epoch': 0.03}\n",
      "{'loss': 3.4611, 'grad_norm': 2.722691059112549, 'learning_rate': 0.0008296296296296296, 'epoch': 0.03}\n",
      "{'loss': 3.5932, 'grad_norm': 3.054668664932251, 'learning_rate': 0.0008259259259259259, 'epoch': 0.03}\n",
      "{'loss': 3.5009, 'grad_norm': 2.994328260421753, 'learning_rate': 0.0008222222222222222, 'epoch': 0.03}\n",
      "{'loss': 3.3385, 'grad_norm': 2.6533803939819336, 'learning_rate': 0.0008185185185185186, 'epoch': 0.03}\n",
      "{'loss': 3.5427, 'grad_norm': 2.919443368911743, 'learning_rate': 0.0008148148148148148, 'epoch': 0.03}\n",
      "{'loss': 3.2875, 'grad_norm': 2.7664248943328857, 'learning_rate': 0.0008111111111111111, 'epoch': 0.03}\n",
      "{'loss': 3.3728, 'grad_norm': 2.758053779602051, 'learning_rate': 0.0008074074074074075, 'epoch': 0.03}\n",
      "{'loss': 3.4458, 'grad_norm': 3.0454039573669434, 'learning_rate': 0.0008037037037037037, 'epoch': 0.03}\n",
      "{'loss': 3.3361, 'grad_norm': 2.9393062591552734, 'learning_rate': 0.0008, 'epoch': 0.03}\n",
      "{'loss': 3.5098, 'grad_norm': 2.787501811981201, 'learning_rate': 0.0007962962962962962, 'epoch': 0.03}\n",
      "{'loss': 3.5682, 'grad_norm': 2.717841386795044, 'learning_rate': 0.0007925925925925927, 'epoch': 0.03}\n",
      "{'loss': 3.2812, 'grad_norm': 2.8651034832000732, 'learning_rate': 0.0007888888888888889, 'epoch': 0.03}\n",
      "{'loss': 3.4539, 'grad_norm': 2.6872942447662354, 'learning_rate': 0.0007851851851851852, 'epoch': 0.03}\n",
      "{'loss': 3.4267, 'grad_norm': 3.035945415496826, 'learning_rate': 0.0007814814814814814, 'epoch': 0.03}\n",
      "{'loss': 3.2614, 'grad_norm': 3.116624593734741, 'learning_rate': 0.0007777777777777778, 'epoch': 0.03}\n",
      "{'loss': 3.4458, 'grad_norm': 3.0747909545898438, 'learning_rate': 0.000774074074074074, 'epoch': 0.03}\n",
      "{'loss': 3.4016, 'grad_norm': 2.963423013687134, 'learning_rate': 0.0007703703703703704, 'epoch': 0.03}\n",
      "{'loss': 3.4596, 'grad_norm': 3.098097801208496, 'learning_rate': 0.0007666666666666667, 'epoch': 0.03}\n",
      "{'loss': 3.5423, 'grad_norm': 2.9879281520843506, 'learning_rate': 0.000762962962962963, 'epoch': 0.03}\n",
      "{'loss': 3.3874, 'grad_norm': 3.0157153606414795, 'learning_rate': 0.0007592592592592593, 'epoch': 0.03}\n",
      "{'loss': 3.4458, 'grad_norm': 3.474686861038208, 'learning_rate': 0.0007555555555555555, 'epoch': 0.03}\n",
      "{'loss': 3.494, 'grad_norm': 2.457949638366699, 'learning_rate': 0.0007518518518518519, 'epoch': 0.03}\n",
      "{'loss': 3.2557, 'grad_norm': 2.69797682762146, 'learning_rate': 0.0007481481481481482, 'epoch': 0.03}\n",
      "{'loss': 3.4089, 'grad_norm': 3.1567671298980713, 'learning_rate': 0.0007444444444444445, 'epoch': 0.03}\n",
      "{'loss': 3.3709, 'grad_norm': 4.404749393463135, 'learning_rate': 0.0007407407407407407, 'epoch': 0.03}\n",
      " 33%|█████████████                          | 1000/3000 [16:35<28:51,  1.15it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 4\n",
      "\n",
      "  0%|                                                    | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      " 15%|██████▊                                     | 2/13 [00:04<00:24,  2.19s/it]\u001b[A\n",
      " 23%|██████████▏                                 | 3/13 [00:20<01:19,  7.90s/it]\u001b[A\n",
      " 31%|█████████████▌                              | 4/13 [00:24<01:00,  6.68s/it]\u001b[A\n",
      " 38%|████████████████▉                           | 5/13 [00:29<00:46,  5.86s/it]\u001b[A\n",
      " 46%|████████████████████▎                       | 6/13 [00:34<00:40,  5.80s/it]\u001b[A\n",
      " 54%|███████████████████████▋                    | 7/13 [00:39<00:32,  5.38s/it]\u001b[A\n",
      " 62%|███████████████████████████                 | 8/13 [00:55<00:43,  8.80s/it]\u001b[A\n",
      " 69%|██████████████████████████████▍             | 9/13 [00:59<00:29,  7.41s/it]\u001b[A\n",
      " 77%|█████████████████████████████████          | 10/13 [01:04<00:19,  6.48s/it]\u001b[A\n",
      " 85%|████████████████████████████████████▍      | 11/13 [01:08<00:11,  5.79s/it]\u001b[A\n",
      " 92%|███████████████████████████████████████▋   | 12/13 [01:11<00:05,  5.06s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 31.632788, 'eval_rouge-2': 7.197154, 'eval_rouge-l': 25.32063, 'eval_bleu-4': 0.031103787061399067, 'eval_runtime': 79.0757, 'eval_samples_per_second': 0.632, 'eval_steps_per_second': 0.164, 'epoch': 0.03}\n",
      " 33%|█████████████                          | 1000/3000 [17:54<28:51,  1.15it/s]\n",
      "100%|███████████████████████████████████████████| 13/13 [01:15<00:00,  4.46s/it]\u001b[A\n",
      "                                                                                \u001b[ACheckpoint destination directory ./output/checkpoint-1000 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Saving model checkpoint to ./output/checkpoint-1000\n",
      "tokenizer config file saved in ./output/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-1000/special_tokens_map.json\n",
      "{'loss': 3.4433, 'grad_norm': 2.904222249984741, 'learning_rate': 0.000737037037037037, 'epoch': 0.04}\n",
      "{'loss': 3.4016, 'grad_norm': 3.023911476135254, 'learning_rate': 0.0007333333333333333, 'epoch': 0.04}\n",
      "{'loss': 3.6502, 'grad_norm': 3.2639408111572266, 'learning_rate': 0.0007296296296296297, 'epoch': 0.04}\n",
      "{'loss': 3.374, 'grad_norm': 2.666079521179199, 'learning_rate': 0.000725925925925926, 'epoch': 0.04}\n",
      "{'loss': 3.3981, 'grad_norm': 2.947751522064209, 'learning_rate': 0.0007222222222222222, 'epoch': 0.04}\n",
      "{'loss': 3.3029, 'grad_norm': 2.8084278106689453, 'learning_rate': 0.0007185185185185186, 'epoch': 0.04}\n",
      "{'loss': 3.3432, 'grad_norm': 2.936387777328491, 'learning_rate': 0.0007148148148148148, 'epoch': 0.04}\n",
      "{'loss': 3.461, 'grad_norm': 2.822702407836914, 'learning_rate': 0.0007111111111111111, 'epoch': 0.04}\n",
      "{'loss': 3.5161, 'grad_norm': 2.856281042098999, 'learning_rate': 0.0007074074074074074, 'epoch': 0.04}\n",
      "{'loss': 3.4428, 'grad_norm': 2.705477714538574, 'learning_rate': 0.0007037037037037038, 'epoch': 0.04}\n",
      "{'loss': 3.3341, 'grad_norm': 3.0531327724456787, 'learning_rate': 0.0007, 'epoch': 0.04}\n",
      "{'loss': 3.4984, 'grad_norm': 2.817349910736084, 'learning_rate': 0.0006962962962962963, 'epoch': 0.04}\n",
      "{'loss': 3.3848, 'grad_norm': 2.9670937061309814, 'learning_rate': 0.0006925925925925925, 'epoch': 0.04}\n",
      "{'loss': 3.2796, 'grad_norm': 2.6851346492767334, 'learning_rate': 0.000688888888888889, 'epoch': 0.04}\n",
      "{'loss': 3.2799, 'grad_norm': 3.097217559814453, 'learning_rate': 0.0006851851851851853, 'epoch': 0.04}\n",
      "{'loss': 3.2913, 'grad_norm': 2.7569665908813477, 'learning_rate': 0.0006814814814814815, 'epoch': 0.04}\n",
      "{'loss': 3.3638, 'grad_norm': 2.8401055335998535, 'learning_rate': 0.0006777777777777778, 'epoch': 0.04}\n",
      "{'loss': 3.4228, 'grad_norm': 2.4125430583953857, 'learning_rate': 0.0006740740740740741, 'epoch': 0.04}\n",
      "{'loss': 3.3202, 'grad_norm': 2.955796003341675, 'learning_rate': 0.0006703703703703704, 'epoch': 0.04}\n",
      "{'loss': 3.3802, 'grad_norm': 2.5432517528533936, 'learning_rate': 0.0006666666666666666, 'epoch': 0.04}\n",
      "{'loss': 3.2177, 'grad_norm': 2.5487937927246094, 'learning_rate': 0.000662962962962963, 'epoch': 0.04}\n",
      "{'loss': 3.3249, 'grad_norm': 2.767775774002075, 'learning_rate': 0.0006592592592592592, 'epoch': 0.04}\n",
      "{'loss': 3.3711, 'grad_norm': 2.8858933448791504, 'learning_rate': 0.0006555555555555556, 'epoch': 0.04}\n",
      "{'loss': 3.2986, 'grad_norm': 2.843492031097412, 'learning_rate': 0.0006518518518518519, 'epoch': 0.04}\n",
      "{'loss': 3.3533, 'grad_norm': 2.575984239578247, 'learning_rate': 0.0006481481481481481, 'epoch': 0.04}\n",
      "{'loss': 3.2505, 'grad_norm': 2.893007278442383, 'learning_rate': 0.0006444444444444444, 'epoch': 0.04}\n",
      "{'loss': 3.4204, 'grad_norm': 2.622605085372925, 'learning_rate': 0.0006407407407407408, 'epoch': 0.04}\n",
      "{'loss': 3.309, 'grad_norm': 2.642014503479004, 'learning_rate': 0.0006370370370370371, 'epoch': 0.04}\n",
      "{'loss': 3.3774, 'grad_norm': 2.4580459594726562, 'learning_rate': 0.0006333333333333333, 'epoch': 0.05}\n",
      "{'loss': 3.434, 'grad_norm': 2.927598714828491, 'learning_rate': 0.0006296296296296296, 'epoch': 0.05}\n",
      "{'loss': 3.4199, 'grad_norm': 2.7111058235168457, 'learning_rate': 0.0006259259259259259, 'epoch': 0.05}\n",
      "{'loss': 3.4146, 'grad_norm': 2.626936674118042, 'learning_rate': 0.0006222222222222223, 'epoch': 0.05}\n",
      "{'loss': 3.3062, 'grad_norm': 3.2391953468322754, 'learning_rate': 0.0006185185185185185, 'epoch': 0.05}\n",
      "{'loss': 3.2742, 'grad_norm': 2.8745265007019043, 'learning_rate': 0.0006148148148148148, 'epoch': 0.05}\n",
      "{'loss': 3.2853, 'grad_norm': 2.852022409439087, 'learning_rate': 0.0006111111111111112, 'epoch': 0.05}\n",
      "{'loss': 3.2734, 'grad_norm': 2.8265645503997803, 'learning_rate': 0.0006074074074074074, 'epoch': 0.05}\n",
      "{'loss': 3.4639, 'grad_norm': 2.6499714851379395, 'learning_rate': 0.0006037037037037037, 'epoch': 0.05}\n",
      "{'loss': 3.3424, 'grad_norm': 2.8542025089263916, 'learning_rate': 0.0006, 'epoch': 0.05}\n",
      "{'loss': 3.3511, 'grad_norm': 2.564687490463257, 'learning_rate': 0.0005962962962962964, 'epoch': 0.05}\n",
      "{'loss': 3.3294, 'grad_norm': 2.5901689529418945, 'learning_rate': 0.0005925925925925926, 'epoch': 0.05}\n",
      "{'loss': 3.2596, 'grad_norm': 3.0617761611938477, 'learning_rate': 0.0005888888888888889, 'epoch': 0.05}\n",
      "{'loss': 3.2221, 'grad_norm': 2.9613757133483887, 'learning_rate': 0.0005851851851851851, 'epoch': 0.05}\n",
      "{'loss': 3.3375, 'grad_norm': 2.8088204860687256, 'learning_rate': 0.0005814814814814815, 'epoch': 0.05}\n",
      "{'loss': 3.2962, 'grad_norm': 2.6875150203704834, 'learning_rate': 0.0005777777777777778, 'epoch': 0.05}\n",
      "{'loss': 3.1882, 'grad_norm': 2.8186721801757812, 'learning_rate': 0.0005740740740740741, 'epoch': 0.05}\n",
      "{'loss': 3.3771, 'grad_norm': 2.778986930847168, 'learning_rate': 0.0005703703703703704, 'epoch': 0.05}\n",
      "{'loss': 3.42, 'grad_norm': 3.506303071975708, 'learning_rate': 0.0005666666666666667, 'epoch': 0.05}\n",
      "{'loss': 3.269, 'grad_norm': 2.45797061920166, 'learning_rate': 0.000562962962962963, 'epoch': 0.05}\n",
      "{'loss': 3.3474, 'grad_norm': 2.936890125274658, 'learning_rate': 0.0005592592592592592, 'epoch': 0.05}\n",
      "{'loss': 3.3725, 'grad_norm': 2.586033821105957, 'learning_rate': 0.0005555555555555556, 'epoch': 0.05}\n",
      " 50%|███████████████████▌                   | 1500/3000 [25:09<20:13,  1.24it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 4\n",
      "\n",
      "  0%|                                                    | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      " 15%|██████▊                                     | 2/13 [00:15<01:27,  7.95s/it]\u001b[A\n",
      " 23%|██████████▏                                 | 3/13 [00:21<01:07,  6.79s/it]\u001b[A\n",
      " 31%|█████████████▌                              | 4/13 [00:25<00:52,  5.88s/it]\u001b[A\n",
      " 38%|████████████████▉                           | 5/13 [00:29<00:42,  5.32s/it]\u001b[A\n",
      " 46%|████████████████████▎                       | 6/13 [00:35<00:38,  5.45s/it]\u001b[A\n",
      " 54%|███████████████████████▋                    | 7/13 [00:40<00:32,  5.36s/it]\u001b[A\n",
      " 62%|███████████████████████████                 | 8/13 [00:45<00:25,  5.13s/it]\u001b[A\n",
      " 69%|██████████████████████████████▍             | 9/13 [00:49<00:19,  4.89s/it]\u001b[A\n",
      " 77%|█████████████████████████████████          | 10/13 [00:53<00:13,  4.62s/it]\u001b[A\n",
      " 85%|████████████████████████████████████▍      | 11/13 [00:57<00:08,  4.38s/it]\u001b[A\n",
      " 92%|███████████████████████████████████████▋   | 12/13 [01:00<00:04,  4.09s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 31.351044, 'eval_rouge-2': 6.855361999999999, 'eval_rouge-l': 25.488287999999997, 'eval_bleu-4': 0.030518418311969216, 'eval_runtime': 69.3211, 'eval_samples_per_second': 0.721, 'eval_steps_per_second': 0.188, 'epoch': 0.05}\n",
      " 50%|███████████████████▌                   | 1500/3000 [26:19<20:13,  1.24it/s]\n",
      "100%|███████████████████████████████████████████| 13/13 [01:04<00:00,  3.94s/it]\u001b[A\n",
      "                                                                                \u001b[ACheckpoint destination directory ./output/checkpoint-1500 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Saving model checkpoint to ./output/checkpoint-1500\n",
      "tokenizer config file saved in ./output/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-1500/special_tokens_map.json\n",
      "{'loss': 3.2868, 'grad_norm': 2.6777000427246094, 'learning_rate': 0.0005518518518518519, 'epoch': 0.05}\n",
      "{'loss': 3.2787, 'grad_norm': 2.743976593017578, 'learning_rate': 0.0005481481481481482, 'epoch': 0.05}\n",
      "{'loss': 3.3869, 'grad_norm': 3.0243513584136963, 'learning_rate': 0.0005444444444444444, 'epoch': 0.05}\n",
      "{'loss': 3.3347, 'grad_norm': 2.3775405883789062, 'learning_rate': 0.0005407407407407407, 'epoch': 0.05}\n",
      "{'loss': 3.4289, 'grad_norm': 2.788560390472412, 'learning_rate': 0.0005370370370370371, 'epoch': 0.05}\n",
      "{'loss': 3.3437, 'grad_norm': 3.030125617980957, 'learning_rate': 0.0005333333333333334, 'epoch': 0.05}\n",
      "{'loss': 3.4215, 'grad_norm': 2.7870941162109375, 'learning_rate': 0.0005296296296296297, 'epoch': 0.05}\n",
      "{'loss': 3.3751, 'grad_norm': 2.978992223739624, 'learning_rate': 0.0005259259259259259, 'epoch': 0.06}\n",
      "{'loss': 3.4192, 'grad_norm': 2.9952352046966553, 'learning_rate': 0.0005222222222222223, 'epoch': 0.06}\n",
      "{'loss': 3.3109, 'grad_norm': 2.6382389068603516, 'learning_rate': 0.0005185185185185185, 'epoch': 0.06}\n",
      "{'loss': 3.3216, 'grad_norm': 2.962918519973755, 'learning_rate': 0.0005148148148148148, 'epoch': 0.06}\n",
      "{'loss': 3.3032, 'grad_norm': 2.926853656768799, 'learning_rate': 0.0005111111111111111, 'epoch': 0.06}\n",
      "{'loss': 3.3759, 'grad_norm': 2.5461294651031494, 'learning_rate': 0.0005074074074074075, 'epoch': 0.06}\n",
      "{'loss': 3.2181, 'grad_norm': 2.901301860809326, 'learning_rate': 0.0005037037037037037, 'epoch': 0.06}\n",
      "{'loss': 3.2901, 'grad_norm': 2.572148084640503, 'learning_rate': 0.0005, 'epoch': 0.06}\n",
      "{'loss': 3.201, 'grad_norm': 2.6893179416656494, 'learning_rate': 0.0004962962962962963, 'epoch': 0.06}\n",
      "{'loss': 3.3906, 'grad_norm': 2.9065680503845215, 'learning_rate': 0.0004925925925925925, 'epoch': 0.06}\n",
      "{'loss': 3.2698, 'grad_norm': 2.722144842147827, 'learning_rate': 0.0004888888888888889, 'epoch': 0.06}\n",
      "{'loss': 3.26, 'grad_norm': 2.825611114501953, 'learning_rate': 0.0004851851851851852, 'epoch': 0.06}\n",
      "{'loss': 3.4376, 'grad_norm': 2.5835540294647217, 'learning_rate': 0.00048148148148148144, 'epoch': 0.06}\n",
      "{'loss': 3.3732, 'grad_norm': 2.7130253314971924, 'learning_rate': 0.0004777777777777778, 'epoch': 0.06}\n",
      "{'loss': 3.3633, 'grad_norm': 2.4870927333831787, 'learning_rate': 0.0004740740740740741, 'epoch': 0.06}\n",
      "{'loss': 3.3038, 'grad_norm': 2.429865598678589, 'learning_rate': 0.0004703703703703704, 'epoch': 0.06}\n",
      "{'loss': 3.301, 'grad_norm': 2.82413387298584, 'learning_rate': 0.00046666666666666666, 'epoch': 0.06}\n",
      "{'loss': 3.3949, 'grad_norm': 2.5907981395721436, 'learning_rate': 0.000462962962962963, 'epoch': 0.06}\n",
      "{'loss': 3.3445, 'grad_norm': 2.693798303604126, 'learning_rate': 0.00045925925925925925, 'epoch': 0.06}\n",
      "{'loss': 3.2969, 'grad_norm': 3.1123716831207275, 'learning_rate': 0.00045555555555555556, 'epoch': 0.06}\n",
      "{'loss': 3.2646, 'grad_norm': 2.9158802032470703, 'learning_rate': 0.00045185185185185183, 'epoch': 0.06}\n",
      "{'loss': 3.2594, 'grad_norm': 2.7287473678588867, 'learning_rate': 0.00044814814814814815, 'epoch': 0.06}\n",
      "{'loss': 3.2651, 'grad_norm': 2.678344488143921, 'learning_rate': 0.0004444444444444444, 'epoch': 0.06}\n",
      "{'loss': 3.2589, 'grad_norm': 2.9134926795959473, 'learning_rate': 0.0004407407407407408, 'epoch': 0.06}\n",
      "{'loss': 3.2736, 'grad_norm': 2.7702934741973877, 'learning_rate': 0.00043703703703703705, 'epoch': 0.06}\n",
      "{'loss': 3.5223, 'grad_norm': 2.887362241744995, 'learning_rate': 0.00043333333333333337, 'epoch': 0.06}\n",
      "{'loss': 3.2492, 'grad_norm': 2.7706806659698486, 'learning_rate': 0.00042962962962962963, 'epoch': 0.06}\n",
      "{'loss': 3.344, 'grad_norm': 3.2178263664245605, 'learning_rate': 0.00042592592592592595, 'epoch': 0.06}\n",
      "{'loss': 3.2671, 'grad_norm': 2.4123687744140625, 'learning_rate': 0.0004222222222222222, 'epoch': 0.06}\n",
      "{'loss': 3.2279, 'grad_norm': 2.8111703395843506, 'learning_rate': 0.00041851851851851853, 'epoch': 0.07}\n",
      "{'loss': 3.2243, 'grad_norm': 3.6815590858459473, 'learning_rate': 0.0004148148148148148, 'epoch': 0.07}\n",
      "{'loss': 3.2926, 'grad_norm': 2.3727517127990723, 'learning_rate': 0.0004111111111111111, 'epoch': 0.07}\n",
      "{'loss': 3.2582, 'grad_norm': 2.70052170753479, 'learning_rate': 0.0004074074074074074, 'epoch': 0.07}\n",
      "{'loss': 3.341, 'grad_norm': 2.926116704940796, 'learning_rate': 0.00040370370370370375, 'epoch': 0.07}\n",
      "{'loss': 3.4067, 'grad_norm': 2.706664562225342, 'learning_rate': 0.0004, 'epoch': 0.07}\n",
      "{'loss': 3.196, 'grad_norm': 2.716827154159546, 'learning_rate': 0.00039629629629629634, 'epoch': 0.07}\n",
      "{'loss': 3.4209, 'grad_norm': 2.430204391479492, 'learning_rate': 0.0003925925925925926, 'epoch': 0.07}\n",
      "{'loss': 3.2322, 'grad_norm': 2.5706536769866943, 'learning_rate': 0.0003888888888888889, 'epoch': 0.07}\n",
      "{'loss': 3.1995, 'grad_norm': 2.831348419189453, 'learning_rate': 0.0003851851851851852, 'epoch': 0.07}\n",
      "{'loss': 3.2708, 'grad_norm': 2.7891697883605957, 'learning_rate': 0.0003814814814814815, 'epoch': 0.07}\n",
      "{'loss': 3.1125, 'grad_norm': 3.203242063522339, 'learning_rate': 0.00037777777777777777, 'epoch': 0.07}\n",
      "{'loss': 3.2925, 'grad_norm': 2.3925821781158447, 'learning_rate': 0.0003740740740740741, 'epoch': 0.07}\n",
      "{'loss': 3.3249, 'grad_norm': 2.861429214477539, 'learning_rate': 0.00037037037037037035, 'epoch': 0.07}\n",
      " 67%|██████████████████████████             | 2000/3000 [33:33<14:06,  1.18it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 4\n",
      "\n",
      "  0%|                                                    | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      " 15%|██████▊                                     | 2/13 [00:04<00:23,  2.13s/it]\u001b[A\n",
      " 23%|██████████▏                                 | 3/13 [00:09<00:32,  3.25s/it]\u001b[A\n",
      " 31%|█████████████▌                              | 4/13 [00:25<01:12,  8.08s/it]\u001b[A\n",
      " 38%|████████████████▉                           | 5/13 [00:28<00:51,  6.48s/it]\u001b[A\n",
      " 46%|████████████████████▎                       | 6/13 [00:35<00:46,  6.63s/it]\u001b[A\n",
      " 54%|███████████████████████▋                    | 7/13 [00:40<00:36,  6.05s/it]\u001b[A\n",
      " 62%|███████████████████████████                 | 8/13 [00:46<00:29,  5.92s/it]\u001b[A\n",
      " 69%|██████████████████████████████▍             | 9/13 [00:50<00:21,  5.42s/it]\u001b[A\n",
      " 77%|█████████████████████████████████          | 10/13 [00:55<00:15,  5.21s/it]\u001b[A\n",
      " 85%|████████████████████████████████████▍      | 11/13 [00:59<00:09,  4.97s/it]\u001b[A\n",
      " 92%|███████████████████████████████████████▋   | 12/13 [01:04<00:04,  4.83s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 33.169222, 'eval_rouge-2': 7.453958, 'eval_rouge-l': 25.436792, 'eval_bleu-4': 0.03534233734534002, 'eval_runtime': 83.9527, 'eval_samples_per_second': 0.596, 'eval_steps_per_second': 0.155, 'epoch': 0.07}\n",
      " 67%|██████████████████████████             | 2000/3000 [34:57<14:06,  1.18it/s]\n",
      "100%|███████████████████████████████████████████| 13/13 [01:07<00:00,  4.40s/it]\u001b[A\n",
      "                                                                                \u001b[ACheckpoint destination directory ./output/checkpoint-2000 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Saving model checkpoint to ./output/checkpoint-2000\n",
      "tokenizer config file saved in ./output/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-2000/special_tokens_map.json\n",
      "{'loss': 3.2833, 'grad_norm': 3.129302978515625, 'learning_rate': 0.00036666666666666667, 'epoch': 0.07}\n",
      "{'loss': 3.3914, 'grad_norm': 2.6308953762054443, 'learning_rate': 0.000362962962962963, 'epoch': 0.07}\n",
      "{'loss': 3.4469, 'grad_norm': 3.0196728706359863, 'learning_rate': 0.0003592592592592593, 'epoch': 0.07}\n",
      "{'loss': 3.3708, 'grad_norm': 2.8877909183502197, 'learning_rate': 0.00035555555555555557, 'epoch': 0.07}\n",
      "{'loss': 3.2186, 'grad_norm': 2.8594090938568115, 'learning_rate': 0.0003518518518518519, 'epoch': 0.07}\n",
      "{'loss': 3.2018, 'grad_norm': 2.5910871028900146, 'learning_rate': 0.00034814814814814816, 'epoch': 0.07}\n",
      "{'loss': 3.3114, 'grad_norm': 2.555142402648926, 'learning_rate': 0.0003444444444444445, 'epoch': 0.07}\n",
      "{'loss': 3.3171, 'grad_norm': 2.6019792556762695, 'learning_rate': 0.00034074074074074074, 'epoch': 0.07}\n",
      "{'loss': 3.3359, 'grad_norm': 2.6648569107055664, 'learning_rate': 0.00033703703703703706, 'epoch': 0.07}\n",
      "{'loss': 3.2594, 'grad_norm': 2.6495556831359863, 'learning_rate': 0.0003333333333333333, 'epoch': 0.07}\n",
      "{'loss': 3.1505, 'grad_norm': 2.6431148052215576, 'learning_rate': 0.0003296296296296296, 'epoch': 0.07}\n",
      "{'loss': 3.478, 'grad_norm': 2.854644536972046, 'learning_rate': 0.00032592592592592596, 'epoch': 0.07}\n",
      "{'loss': 3.1313, 'grad_norm': 2.4497389793395996, 'learning_rate': 0.0003222222222222222, 'epoch': 0.07}\n",
      "{'loss': 3.2626, 'grad_norm': 2.787644386291504, 'learning_rate': 0.00031851851851851854, 'epoch': 0.07}\n",
      "{'loss': 3.2582, 'grad_norm': 2.518355131149292, 'learning_rate': 0.0003148148148148148, 'epoch': 0.08}\n",
      "{'loss': 3.3762, 'grad_norm': 2.8148200511932373, 'learning_rate': 0.0003111111111111111, 'epoch': 0.08}\n",
      "{'loss': 3.259, 'grad_norm': 2.447875499725342, 'learning_rate': 0.0003074074074074074, 'epoch': 0.08}\n",
      "{'loss': 3.2949, 'grad_norm': 2.661637306213379, 'learning_rate': 0.0003037037037037037, 'epoch': 0.08}\n",
      "{'loss': 3.2144, 'grad_norm': 2.7103378772735596, 'learning_rate': 0.0003, 'epoch': 0.08}\n",
      "{'loss': 3.2939, 'grad_norm': 2.498908281326294, 'learning_rate': 0.0002962962962962963, 'epoch': 0.08}\n",
      "{'loss': 3.3291, 'grad_norm': 2.5337703227996826, 'learning_rate': 0.00029259259259259256, 'epoch': 0.08}\n",
      "{'loss': 3.295, 'grad_norm': 2.4429702758789062, 'learning_rate': 0.0002888888888888889, 'epoch': 0.08}\n",
      "{'loss': 3.2765, 'grad_norm': 2.884382963180542, 'learning_rate': 0.0002851851851851852, 'epoch': 0.08}\n",
      "{'loss': 3.2448, 'grad_norm': 2.7114906311035156, 'learning_rate': 0.0002814814814814815, 'epoch': 0.08}\n",
      "{'loss': 3.1145, 'grad_norm': 3.2089955806732178, 'learning_rate': 0.0002777777777777778, 'epoch': 0.08}\n",
      "{'loss': 3.263, 'grad_norm': 2.688394546508789, 'learning_rate': 0.0002740740740740741, 'epoch': 0.08}\n",
      "{'loss': 3.2913, 'grad_norm': 2.9746387004852295, 'learning_rate': 0.00027037037037037036, 'epoch': 0.08}\n",
      "{'loss': 3.3127, 'grad_norm': 2.530900239944458, 'learning_rate': 0.0002666666666666667, 'epoch': 0.08}\n",
      "{'loss': 3.1595, 'grad_norm': 2.7160227298736572, 'learning_rate': 0.00026296296296296294, 'epoch': 0.08}\n",
      "{'loss': 3.211, 'grad_norm': 2.6042251586914062, 'learning_rate': 0.00025925925925925926, 'epoch': 0.08}\n",
      "{'loss': 3.1684, 'grad_norm': 2.6847541332244873, 'learning_rate': 0.00025555555555555553, 'epoch': 0.08}\n",
      "{'loss': 3.2185, 'grad_norm': 2.959080696105957, 'learning_rate': 0.00025185185185185185, 'epoch': 0.08}\n",
      "{'loss': 3.2094, 'grad_norm': 2.7210443019866943, 'learning_rate': 0.00024814814814814816, 'epoch': 0.08}\n",
      "{'loss': 3.2253, 'grad_norm': 2.5207409858703613, 'learning_rate': 0.00024444444444444443, 'epoch': 0.08}\n",
      "{'loss': 3.1212, 'grad_norm': 2.845874071121216, 'learning_rate': 0.00024074074074074072, 'epoch': 0.08}\n",
      "{'loss': 3.2654, 'grad_norm': 2.5989341735839844, 'learning_rate': 0.00023703703703703704, 'epoch': 0.08}\n",
      "{'loss': 3.1839, 'grad_norm': 2.924025535583496, 'learning_rate': 0.00023333333333333333, 'epoch': 0.08}\n",
      "{'loss': 3.3408, 'grad_norm': 2.837378740310669, 'learning_rate': 0.00022962962962962962, 'epoch': 0.08}\n",
      "{'loss': 3.0942, 'grad_norm': 2.7359557151794434, 'learning_rate': 0.00022592592592592591, 'epoch': 0.08}\n",
      "{'loss': 3.3063, 'grad_norm': 2.4664602279663086, 'learning_rate': 0.0002222222222222222, 'epoch': 0.08}\n",
      "{'loss': 3.3208, 'grad_norm': 2.673931121826172, 'learning_rate': 0.00021851851851851852, 'epoch': 0.08}\n",
      "{'loss': 3.1156, 'grad_norm': 2.394193410873413, 'learning_rate': 0.00021481481481481482, 'epoch': 0.08}\n",
      "{'loss': 3.2466, 'grad_norm': 2.312248468399048, 'learning_rate': 0.0002111111111111111, 'epoch': 0.08}\n",
      "{'loss': 3.2309, 'grad_norm': 2.575255870819092, 'learning_rate': 0.0002074074074074074, 'epoch': 0.09}\n",
      "{'loss': 3.1257, 'grad_norm': 2.503519058227539, 'learning_rate': 0.0002037037037037037, 'epoch': 0.09}\n",
      "{'loss': 3.1651, 'grad_norm': 2.5156185626983643, 'learning_rate': 0.0002, 'epoch': 0.09}\n",
      "{'loss': 3.1485, 'grad_norm': 2.5934085845947266, 'learning_rate': 0.0001962962962962963, 'epoch': 0.09}\n",
      "{'loss': 3.2875, 'grad_norm': 2.2988362312316895, 'learning_rate': 0.0001925925925925926, 'epoch': 0.09}\n",
      "{'loss': 3.346, 'grad_norm': 3.0525009632110596, 'learning_rate': 0.00018888888888888888, 'epoch': 0.09}\n",
      "{'loss': 3.2327, 'grad_norm': 3.3205220699310303, 'learning_rate': 0.00018518518518518518, 'epoch': 0.09}\n",
      " 83%|████████████████████████████████▌      | 2500/3000 [42:14<07:02,  1.18it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 4\n",
      "\n",
      "  0%|                                                    | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      " 15%|██████▊                                     | 2/13 [00:15<01:27,  7.94s/it]\u001b[A\n",
      " 23%|██████████▏                                 | 3/13 [00:19<01:01,  6.16s/it]\u001b[A\n",
      " 31%|█████████████▌                              | 4/13 [00:35<01:29,  9.90s/it]\u001b[A\n",
      " 38%|████████████████▉                           | 5/13 [00:40<01:05,  8.13s/it]\u001b[A\n",
      " 46%|████████████████████▎                       | 6/13 [00:46<00:50,  7.27s/it]\u001b[A\n",
      " 54%|███████████████████████▋                    | 7/13 [00:54<00:46,  7.71s/it]\u001b[A\n",
      " 62%|███████████████████████████                 | 8/13 [01:02<00:38,  7.71s/it]\u001b[A\n",
      " 69%|██████████████████████████████▍             | 9/13 [01:07<00:26,  6.75s/it]\u001b[A\n",
      " 77%|█████████████████████████████████          | 10/13 [01:12<00:18,  6.30s/it]\u001b[A\n",
      " 85%|████████████████████████████████████▍      | 11/13 [01:16<00:11,  5.71s/it]\u001b[A\n",
      " 92%|███████████████████████████████████████▋   | 12/13 [01:20<00:05,  5.22s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 32.903904000000004, 'eval_rouge-2': 7.98487, 'eval_rouge-l': 25.140842, 'eval_bleu-4': 0.038046572591531234, 'eval_runtime': 88.7607, 'eval_samples_per_second': 0.563, 'eval_steps_per_second': 0.146, 'epoch': 0.09}\n",
      " 83%|████████████████████████████████▌      | 2500/3000 [43:43<07:02,  1.18it/s]\n",
      "100%|███████████████████████████████████████████| 13/13 [01:23<00:00,  4.55s/it]\u001b[A\n",
      "                                                                                \u001b[ACheckpoint destination directory ./output/checkpoint-2500 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Saving model checkpoint to ./output/checkpoint-2500\n",
      "tokenizer config file saved in ./output/checkpoint-2500/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-2500/special_tokens_map.json\n",
      "{'loss': 3.1395, 'grad_norm': 2.6297805309295654, 'learning_rate': 0.0001814814814814815, 'epoch': 0.09}\n",
      "{'loss': 3.1735, 'grad_norm': 2.7389278411865234, 'learning_rate': 0.00017777777777777779, 'epoch': 0.09}\n",
      "{'loss': 3.096, 'grad_norm': 2.63798189163208, 'learning_rate': 0.00017407407407407408, 'epoch': 0.09}\n",
      "{'loss': 3.2399, 'grad_norm': 3.1339986324310303, 'learning_rate': 0.00017037037037037037, 'epoch': 0.09}\n",
      "{'loss': 3.2176, 'grad_norm': 2.481740951538086, 'learning_rate': 0.00016666666666666666, 'epoch': 0.09}\n",
      "{'loss': 3.191, 'grad_norm': 2.6597843170166016, 'learning_rate': 0.00016296296296296298, 'epoch': 0.09}\n",
      "{'loss': 3.3374, 'grad_norm': 2.562682628631592, 'learning_rate': 0.00015925925925925927, 'epoch': 0.09}\n",
      "{'loss': 3.304, 'grad_norm': 2.8048112392425537, 'learning_rate': 0.00015555555555555556, 'epoch': 0.09}\n",
      "{'loss': 3.2561, 'grad_norm': 2.9205522537231445, 'learning_rate': 0.00015185185185185185, 'epoch': 0.09}\n",
      "{'loss': 3.3467, 'grad_norm': 2.6681864261627197, 'learning_rate': 0.00014814814814814815, 'epoch': 0.09}\n",
      "{'loss': 3.1936, 'grad_norm': 2.4435904026031494, 'learning_rate': 0.00014444444444444444, 'epoch': 0.09}\n",
      "{'loss': 3.2723, 'grad_norm': 2.457780361175537, 'learning_rate': 0.00014074074074074076, 'epoch': 0.09}\n",
      "{'loss': 3.3311, 'grad_norm': 2.587592363357544, 'learning_rate': 0.00013703703703703705, 'epoch': 0.09}\n",
      "{'loss': 3.27, 'grad_norm': 2.91467022895813, 'learning_rate': 0.00013333333333333334, 'epoch': 0.09}\n",
      "{'loss': 3.236, 'grad_norm': 2.3868398666381836, 'learning_rate': 0.00012962962962962963, 'epoch': 0.09}\n",
      "{'loss': 3.1494, 'grad_norm': 2.424220085144043, 'learning_rate': 0.00012592592592592592, 'epoch': 0.09}\n",
      "{'loss': 3.2502, 'grad_norm': 2.700242280960083, 'learning_rate': 0.00012222222222222221, 'epoch': 0.09}\n",
      "{'loss': 3.1119, 'grad_norm': 2.3966894149780273, 'learning_rate': 0.00011851851851851852, 'epoch': 0.09}\n",
      "{'loss': 3.3112, 'grad_norm': 2.6999406814575195, 'learning_rate': 0.00011481481481481481, 'epoch': 0.09}\n",
      "{'loss': 3.2772, 'grad_norm': 2.724457025527954, 'learning_rate': 0.0001111111111111111, 'epoch': 0.09}\n",
      "{'loss': 3.2518, 'grad_norm': 2.651745557785034, 'learning_rate': 0.00010740740740740741, 'epoch': 0.09}\n",
      "{'loss': 3.104, 'grad_norm': 2.4304044246673584, 'learning_rate': 0.0001037037037037037, 'epoch': 0.09}\n",
      "{'loss': 3.2404, 'grad_norm': 2.4883971214294434, 'learning_rate': 0.0001, 'epoch': 0.1}\n",
      "{'loss': 3.2066, 'grad_norm': 2.4014203548431396, 'learning_rate': 9.62962962962963e-05, 'epoch': 0.1}\n",
      "{'loss': 3.2844, 'grad_norm': 2.6291518211364746, 'learning_rate': 9.259259259259259e-05, 'epoch': 0.1}\n",
      "{'loss': 3.2216, 'grad_norm': 2.5874674320220947, 'learning_rate': 8.888888888888889e-05, 'epoch': 0.1}\n",
      "{'loss': 3.1867, 'grad_norm': 2.5009195804595947, 'learning_rate': 8.518518518518518e-05, 'epoch': 0.1}\n",
      "{'loss': 3.0841, 'grad_norm': 2.638768196105957, 'learning_rate': 8.148148148148149e-05, 'epoch': 0.1}\n",
      "{'loss': 3.143, 'grad_norm': 2.3093066215515137, 'learning_rate': 7.777777777777778e-05, 'epoch': 0.1}\n",
      "{'loss': 3.0765, 'grad_norm': 2.494800567626953, 'learning_rate': 7.407407407407407e-05, 'epoch': 0.1}\n",
      "{'loss': 3.2891, 'grad_norm': 2.555687189102173, 'learning_rate': 7.037037037037038e-05, 'epoch': 0.1}\n",
      "{'loss': 3.2122, 'grad_norm': 2.3806345462799072, 'learning_rate': 6.666666666666667e-05, 'epoch': 0.1}\n",
      "{'loss': 3.1749, 'grad_norm': 2.6668243408203125, 'learning_rate': 6.296296296296296e-05, 'epoch': 0.1}\n",
      "{'loss': 3.2227, 'grad_norm': 2.6410744190216064, 'learning_rate': 5.925925925925926e-05, 'epoch': 0.1}\n",
      "{'loss': 3.2578, 'grad_norm': 2.65734601020813, 'learning_rate': 5.555555555555555e-05, 'epoch': 0.1}\n",
      "{'loss': 3.201, 'grad_norm': 2.5725653171539307, 'learning_rate': 5.185185185185185e-05, 'epoch': 0.1}\n",
      "{'loss': 3.1914, 'grad_norm': 2.7180933952331543, 'learning_rate': 4.814814814814815e-05, 'epoch': 0.1}\n",
      "{'loss': 3.3092, 'grad_norm': 2.8002383708953857, 'learning_rate': 4.4444444444444447e-05, 'epoch': 0.1}\n",
      "{'loss': 3.0855, 'grad_norm': 2.4920461177825928, 'learning_rate': 4.0740740740740745e-05, 'epoch': 0.1}\n",
      "{'loss': 3.1418, 'grad_norm': 3.0056097507476807, 'learning_rate': 3.7037037037037037e-05, 'epoch': 0.1}\n",
      "{'loss': 3.155, 'grad_norm': 2.3818089962005615, 'learning_rate': 3.3333333333333335e-05, 'epoch': 0.1}\n",
      "{'loss': 3.0204, 'grad_norm': 2.2845041751861572, 'learning_rate': 2.962962962962963e-05, 'epoch': 0.1}\n",
      "{'loss': 3.1759, 'grad_norm': 2.682887077331543, 'learning_rate': 2.5925925925925925e-05, 'epoch': 0.1}\n",
      "{'loss': 3.048, 'grad_norm': 2.422106981277466, 'learning_rate': 2.2222222222222223e-05, 'epoch': 0.1}\n",
      "{'loss': 3.1746, 'grad_norm': 2.54594087600708, 'learning_rate': 1.8518518518518518e-05, 'epoch': 0.1}\n",
      "{'loss': 3.001, 'grad_norm': 3.1284067630767822, 'learning_rate': 1.4814814814814815e-05, 'epoch': 0.1}\n",
      "{'loss': 3.2517, 'grad_norm': 2.963306188583374, 'learning_rate': 1.1111111111111112e-05, 'epoch': 0.1}\n",
      "{'loss': 3.2554, 'grad_norm': 2.9044504165649414, 'learning_rate': 7.4074074074074075e-06, 'epoch': 0.1}\n",
      "{'loss': 3.3098, 'grad_norm': 2.444849967956543, 'learning_rate': 3.7037037037037037e-06, 'epoch': 0.1}\n",
      "{'loss': 3.1335, 'grad_norm': 2.2448136806488037, 'learning_rate': 0.0, 'epoch': 0.1}\n",
      "100%|███████████████████████████████████████| 3000/3000 [50:58<00:00,  1.20it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 4\n",
      "\n",
      "  0%|                                                    | 0/13 [00:00<?, ?it/s]\u001b[A\n",
      " 15%|██████▊                                     | 2/13 [00:05<00:30,  2.73s/it]\u001b[A\n",
      " 23%|██████████▏                                 | 3/13 [00:09<00:33,  3.30s/it]\u001b[A\n",
      " 31%|█████████████▌                              | 4/13 [00:15<00:37,  4.13s/it]\u001b[A\n",
      " 38%|████████████████▉                           | 5/13 [00:19<00:35,  4.39s/it]\u001b[A\n",
      " 46%|████████████████████▎                       | 6/13 [00:25<00:33,  4.76s/it]\u001b[A\n",
      " 54%|███████████████████████▋                    | 7/13 [00:30<00:29,  4.97s/it]\u001b[A\n",
      " 62%|███████████████████████████                 | 8/13 [00:36<00:26,  5.32s/it]\u001b[A\n",
      " 69%|██████████████████████████████▍             | 9/13 [00:40<00:19,  4.87s/it]\u001b[A\n",
      " 77%|█████████████████████████████████          | 10/13 [00:44<00:13,  4.63s/it]\u001b[A\n",
      " 85%|████████████████████████████████████▍      | 11/13 [00:50<00:09,  4.84s/it]\u001b[A\n",
      " 92%|███████████████████████████████████████▋   | 12/13 [00:53<00:04,  4.51s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 33.627708000000005, 'eval_rouge-2': 7.844266, 'eval_rouge-l': 25.931263999999995, 'eval_bleu-4': 0.0345336534347075, 'eval_runtime': 62.5308, 'eval_samples_per_second': 0.8, 'eval_steps_per_second': 0.208, 'epoch': 0.1}\n",
      "100%|███████████████████████████████████████| 3000/3000 [52:00<00:00,  1.20it/s]\n",
      "100%|███████████████████████████████████████████| 13/13 [00:58<00:00,  4.39s/it]\u001b[A\n",
      "                                                                                \u001b[ACheckpoint destination directory ./output/checkpoint-3000 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
      "Saving model checkpoint to ./output/checkpoint-3000\n",
      "tokenizer config file saved in ./output/checkpoint-3000/tokenizer_config.json\n",
      "Special tokens file saved in ./output/checkpoint-3000/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 3120.917, 'train_samples_per_second': 3.845, 'train_steps_per_second': 0.961, 'train_loss': 3.368067057291667, 'epoch': 0.1}\n",
      "100%|███████████████████████████████████████| 3000/3000 [52:00<00:00,  1.04s/it]\n",
      "***** Running Prediction *****\n",
      "  Num examples = 1070\n",
      "  Batch size = 4\n",
      "100%|█████████████████████████████████████████| 268/268 [22:44<00:00,  5.09s/it]\n"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=0 /root/miniconda3/envs/chatglm/bin/python3 finetune_hf.py  data/AdvertiseGen_fix  /root/models/chatglm3-6b  configs/lora.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9418f6c5c264601",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 3. 使用微调的数据集进行推理\n",
    "在完成微调任务之后，我们可以查看到 `output` 文件夹下多了很多个`checkpoint-*`的文件夹，这些文件夹代表了训练的轮数。\n",
    "我们选择最后一轮的微调权重，并使用inference进行导入。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f22b735175e1c0d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-18T07:03:19.390123Z",
     "start_time": "2024-01-18T07:03:19.246666Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint-1000  checkpoint-2000  checkpoint-3000\n",
      "checkpoint-1500  checkpoint-2500  checkpoint-500\n"
     ]
    }
   ],
   "source": [
    "!ls output/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5060015c24e97ae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-18T07:08:13.616364Z",
     "start_time": "2024-01-18T07:07:07.346906Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████████████| 7/7 [00:06<00:00,  1.11it/s]\n",
      "Setting eos_token is not supported, use the default one.\n",
      "Setting pad_token is not supported, use the default one.\n",
      "Setting unk_token is not supported, use the default one.\n",
      "套头式连衣裙，方便穿脱，适合各种身材的宝宝。网纱拼接，增加层次感，更显甜美。前片压褶百褶设计，增加裙子的层次感，增加裙子的立体感。袖口和领口木耳边点缀，凸显甜美可爱。下摆不规则压褶，显瘦显高。背部的拉链设计，方便穿脱，增加裙子的细节感。\n"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=0  /root/miniconda3/envs/chatglm/bin/python3 inference_hf.py  /root/ChatGLM3/finetune_demo/output/checkpoint-3000/ --prompt \"类型#裙*版型#显瘦*材质#网纱*风格#性感*裙型#百褶*裙下摆#压褶*裙长#连衣裙*裙衣门襟#拉链*裙衣门襟#套头*裙款式#拼接*裙款式#拉链*裙款式#木耳边*裙款式#抽褶*裙款式#不规则\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18cd83087f096094",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 4. 总结\n",
    "到此位置，我们就完成了使用单张 GPU Lora 来微调 ChatGLM3-6B 模型，使其能生产出更好的广告。\n",
    "在本章节中，你将会学会：\n",
    "+ 如何使用模型进行 Lora 微调\n",
    "+ 微调数据集的准备和对齐\n",
    "+ 使用微调的模型进行推理"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
