data_config:
  train_file: train.json
  val_file: dev.json
  test_file: dev.json
  num_proc: 8
max_input_length: 128
max_output_length: 256
training_args:
  # see `transformers.Seq2SeqTrainingArguments`
  output_dir: ./output
  max_steps: 3000

  # 增加一些修改，优化微调过程：zf
  gradient_accumulation_steps: 4 # 增加梯度累积以模拟更大批次大小
  lr_scheduler_type: "linear" # 学习率调度器类型
  learning_rate: 1e-3  # 调整学习率，可能需要根据训练情况进一步调整
  warmup_ratio: 0.1    # 增加预热比例 

  # settings for data loading
  per_device_train_batch_size: 1
  dataloader_num_workers: 8 #16->8
  remove_unused_columns: false
  # settings for saving checkpoints
  save_strategy: steps
  save_steps: 500
  # settings for logging
  log_level: info
  logging_strategy: steps
  logging_steps: 10
  # settings for evaluation
  per_device_eval_batch_size: 4 #16->4
  evaluation_strategy: steps
  eval_steps: 500
  # settings for optimizer
  # adam_epsilon: 1e-6
  # uncomment the following line to detect nan or inf values
  # debug: underflow_overflow
  predict_with_generate: true

  # see `transformers.GenerationConfig`
  generation_config:
    max_new_tokens: 256
  # set your absolute deepspeed path here
  #deepspeed: ds_zero_2.json
  # set to true if train with cpu.
  use_cpu: false
peft_config:
  peft_type: LORA
  task_type: CAUSAL_LM
  r: 8 
  lora_alpha: 32
  lora_dropout: 0.1
